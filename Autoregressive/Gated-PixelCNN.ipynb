{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0fvK3Rgac3X"
   },
   "source": [
    "# Review: Conditional Image Generation with PixelCNN Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jvt_q5jKL2y"
   },
   "source": [
    "## Paper Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS5vzHkTbVkn"
   },
   "source": [
    "### Prior Reseach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce03Q4Eit6p-"
   },
   "source": [
    "+ image distribution을 특정 pixel에서부터 dependency가 있다고 가정하면 seqential하게 conditional distribution으로 표현 가능함   \n",
    "\n",
    "$$ p(\\mathbf{x}) = \\prod^{n^2}_{i=1}p(x_i|x_1, ... , x_{i-1}), \\, \\mathbf{x} \\in \\mathbb{R}^{n \\times n}$$\n",
    "\n",
    "+ 이러한 concept를 구현한 Autoregressive generative model은 PixelRNN(Diagonal BiLSTM)으로 성공적인 접근법임을 보였음  \n",
    "+ 특히 explict하게 distribution이 표현되고, model이 diverse하게 distribution을 generate 할 수 있다는 점은 고유의 장점\n",
    "+ 그러나 PixelRNN은 BiLSTM 구조를 사용하였기 때문에 병렬화에 불리했으며 inference time이 너무 느렸음\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759587-e5dc5ef6-6790-4fa4-a904-b55ecbec5b3f.JPG\" alt=\"1\" width=\"400px\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXzIbXJXxdqe"
   },
   "source": [
    "+ 이 때 PixelRNN에 Diagnoal BiLSTM 구조보다 성능이 떨어졌지만, Convolution layer를 활용한 **PixelCNN** 또한 제안되었음\n",
    "+ Convolution layer는 RNN보다 병렬화에 유리하기 때문에, 이를 개선할 수 있다면 PixelRNN의 단점을 극복이 가능함  \n",
    "+ 이를 위해 저자는 **Gated PixelCNN** architecture를 제안\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759591-348cdb0c-c84c-4bc0-9f1d-f67beb5c00e3.JPG\" alt=\"2\" width=\"600px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b06BfxUmyiyZ"
   },
   "source": [
    "### Problem of PixelCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kxb7ra9DzFzO"
   },
   "source": [
    "+ PixelCNN은 raseter scan sequence로 pixel 간의 dependency가 있다고 가정한 모델\n",
    "+ 따라서 raster scan sequence 상 이전 pixel만을 covolution하는 **Masked Convolution**으로 이를 구현함 \n",
    "+ Masking을 통해서 generate하려는 pixel 이후의 sequence에 대해서는 반영하지 않게 하여 dependece를 구현  \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759593-6ab9412c-978f-4a01-a91b-c1f05d66db23.JPG\" alt=\"3\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "+ 그러나 이러한 Masked Convolution을 사용하는 PixelCNN은 3가지 problem이 존재\n",
    "1. 한 pixel에 대해서 BiLSTM보다 상대적으로 짧은 dependecy length를 가짐  \n",
    "→ BiLSTM은 previous sequence가 hidden input으로 반영되지만, convolution을 사용할 시에는 neihborhood pixel만이 반영됨\n",
    "2. Masked Convolution은 본질적으로 target pixel의 우상향 대각선으로 **Blind Spot**이 발생함  \n",
    "→ 아무리 layer를 stack해도 Masking의 형태 상 local receptive field에서 반영되지 않는 previous sequence가 존재하게 됨\n",
    "3. BiLSTM은 gate가 존재하여 보다 복잡한 nonlinearity를 학습할 수 있음  \n",
    "→ Convolution layer는 output으로 하나의 activation이 작용하지만, BiLSTM은 forget, input, output마다 다른 activation이 적용됨\n",
    "\n",
    "\n",
    "|Masked Convolution|Result|\n",
    "|:-:|:-:|\n",
    "| <img src=\"https://user-images.githubusercontent.com/86907286/161759594-fdd2ccbc-43c0-4856-9a4f-a8d36d8fcdff.gif\" alt=\"4\" width=\"200px\"/> | <img src=\"https://user-images.githubusercontent.com/86907286/161759596-c2bea2d9-d985-4cd5-a1b7-cc026f9a6788.png\" alt=\"5\" width=\"200px\"/>|\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759599-5f9e6ac9-3151-4a0b-919f-d4d93397374e.JPG\" alt=\"6\" width=\"400px\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z3B2P-k6E0n"
   },
   "source": [
    "### Vertical Stack, Horizontal Stack\n",
    "\n",
    "+ Problem 2을 위해 Masked CNN이 아닌 **Vertical Stack**과 **Horizontal Stack**으로 구성된 2개의 sperated layer를 결합하는 방법을 제안\n",
    "+ 특정한 pixel을 generate할 때 해당 pixel과 dependency가 존재하는 pixel는 2가지로 나눌 수 있음\n",
    "  1. 같은 row에 있는 previous sequence에 해당하는 pixel\n",
    "  2. 이전 row에 있는 모든 pixel\n",
    "+ 각 유형만을 rocal receptive field로 cover하는 두 종류의 convolution filter를 사용하여 각각의 feature map을 결합\n",
    "+ 이 때 중요한 것은  **각 pixel의 dependecy를 유지**하는 것!\n",
    "  \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759602-398bd9c0-c0f6-4916-be92-4dc4c25081a0.JPG\" alt=\"8\" width=\"200px\" align=\"center\" />\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759605-58a6bc3b-d939-441c-b0a2-67daa49f8503.JPG\" alt=\"9\" width=\"400px\" align=\"center\" />\n",
    "\n",
    "+ 3x3 size convolution을 Masked Convolution을 사용할 경우의 filter size라고 가정\n",
    "+ 이 때, Vertical Stack은 2x1, Horizontal Stack은 3x2 filter로 표현될 수 있음\n",
    "+ 그러나 원본 feature map에 바로 두 filter를 적용할 시에 Vertical convolution은 **다음 sequence pixel의 정보**를 참고함  \n",
    "+ 이는 image distribution이 각 pixel의 **conditional sequence로 표현될 수 있다는 가정에서 위배됨**\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759610-4746ea38-2369-4935-8381-93ae0d552f62.JPG\" alt=\"10\" width=\"300px\" align=\"center\" />\n",
    "\n",
    "+ 따라서 Verical Stack filter에 대해서는 **input feature map의 상단에 1 row padding을 추가**  \n",
    "+ Horizontal Stack filter가 적용되는 pixel index의 Verical Stack filter의 feature map pixel은 **1 row 과거가 됨**\n",
    "+ 이렇게 진행 후 최종 Vertical Stack feature map에서 **최하단 1 row를 crop**\n",
    "+ 따라서 Vertical Stack feature map의 각 pixel은 **원본 feature map pixel의 vertical local dependecy를 표현하게 됨**\n",
    "+ 이렇게 얻은 두 feature map을 결합하게 되면 **Blind Point**이 더 이상 발생하지 않음\n",
    "+ 동시에 masking을 통해 매번 불필요한 convolution이 사라지는 효과를 통해 efficient하게 model을 구성 가능\n",
    "\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759612-fb52eb3a-0f44-4de9-8b54-70ae97b1ff0e.JPG\" alt=\"11\" width=\"700px\" align=\"center\" />\n",
    "\n",
    "|Masked Convolution|Vertical/Horizontal Stack|\n",
    "|:-:|:-:|\n",
    "| <img src=\"https://user-images.githubusercontent.com/86907286/161759567-07ca3354-c724-4017-b530-802e46ef11ed.gif\" alt=\"11-1\" width=\"300px\" /> | <img src=\"https://user-images.githubusercontent.com/86907286/161759570-bdfcf60d-9357-4e9c-941f-53e8337d26a2.gif\" alt=\"11-2\" width=\"300px\" />|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH2CGmlR4QGZ"
   },
   "source": [
    "### Gated Convolutional Layers\n",
    "\n",
    "+ problem 1에 대해서 저자는 layer의 많은 stack을 통해서 local receptive field를 넓게 확보하면 극복할 수 있다고 설명\n",
    "+ 그러나 problem 3은 Masked Convolution의 근본적인 문제이기 때문에 개선이 필요함  \n",
    "+ 따라서 저자는 covolution이 적용된 후의 feature map을 두개의 flow로 만들어 서로 다른 activation을 적용\n",
    "+ 하나는 **feature map의 선택을 담당하도록 sigmoid**, 다른 하나는 **feature map의 normalize를 위해 tanh** 적용  \n",
    "+ 이후 element-wise product을 통해서 feature map의 특정 element를 augment/diminish 할 수 있도록 구성\n",
    "\n",
    "$$ \\mathbf{y} = \\text{tanh}(W_{k, f} * \\mathbf{x}) \\odot \\sigma (W_{k,g} * \\mathbf{x}) $$\n",
    "\n",
    "+ 이러한 gate operation은 Vertical stack이 horizontal stack에 반영되기 전에 적용됨\n",
    "+ **gate operation 이전에 1x1 convolution을 통해서 Vertical flow를 Horizontal flow에 반영**\n",
    "+ 이 때 Horizontal flow는 첫번째 layer를 제외하고 **residual connection**이 적용되며, 최종 prediction에 사용되는 flow\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759573-7565968c-888a-4d0f-9c6d-cdffa787c689.JPG\" alt=\"12\" width=\"500px\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avOADelRKD9M"
   },
   "source": [
    "### Conditional PixelCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWtpUUnUKdZ2"
   },
   "source": [
    "+ 지금까지 살펴 본 구조는 uncoditional한 조건에서의 generative model이었음\n",
    "+ 만약 적절한 condition 정보가 주어진 상황에서의 distribution을 알고 싶다면 **latent vector $\\mathbf{h}$를 도입**하여 이를 표현 가능\n",
    "\n",
    "\n",
    "$$ p(\\mathbf{x}|\\mathbf{h}) = \\prod^{n^2}_{i=1}p(x_i|x_1, ... , x_{i-1}, \\mathbf{h}), \\, \\mathbf{x} \\in \\mathbb{R}^{n \\times n}, \\, \\mathbf{h} \\in \\mathbb{R}^d$$\n",
    "\n",
    "+ 저자는 이를 각 Gated Convolution Layer에서 gated activation을 적용하기 전에 latent vector를 더해주는 것으로 구현하였음  \n",
    "+ 이 때, 각 layer의 dimension을 맞추기 위한 적절한 transform $V$를 적용\n",
    "\n",
    "$$ \\mathbf{y} = \\text{tanh}(W_{k, f} * \\mathbf{x} + V^T_{k,f} \\cdot \\mathbf{h}) \\odot \\sigma (W_{k,g} * \\mathbf{x} + V^T_{k,g} \\cdot \\mathbf{h}) $$\n",
    "\n",
    "+ 만약 $\\mathbf{h}$가 class를 표현하는 one-hot vector라면 이는 class 정보를 담는 bias가 됨  \n",
    "+ 그러나 이는 **보다 복잡한 condition, 예를 들어 position이나 pose에 대한 정보가 부족**\n",
    "+ 따라서 이러한 정보가 embedding된 representation $\\mathbf{s}$를 생성 할 수 있다면 보다 복잡한 representation을 표현 가능\n",
    "+ 이를 생성할 수 있는 decovolution neural network $\\mathbf{s} = m(\\mathbf{h})$를 사용하면 다음과 같이 표현할 수 있음\n",
    "\n",
    "$$ \\mathbf{y} = \\text{tanh}(W_{k, f} * \\mathbf{x} + V^T_{k,f} * \\mathbf{s}) \\odot \\sigma (W_{k,g} * \\mathbf{x} + V^T_{k,g} * \\mathbf{s}) $$\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759576-7ed0e29e-de3b-4216-9227-df8307f11d97.JPG\" alt=\"13\" width=\"700px\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBfkVTb5ODn8"
   },
   "source": [
    "### PixelCNN as /Auto-Encoders\n",
    "\n",
    "+ PixelCNN은 diverse하게 distribution을 generate 할 수 있으면서 latent vector를 통해 multimodal하게 generate 가능\n",
    "+ 이를 활용하면 PixelCNN을 **Auto-Encoder의 decoder로서 활용할 수 있음**  \n",
    "+ 이에 대한 실험의 결과를 보면 **latent vector에 반응하는 양상이 conventinal Auto-Encoder와 달랐음**\n",
    "+ 일반적인 Auto-Encoder는 reconstruction을 시도하지만 **PixelCNN decoder는 유사한 image를 generate하려고 시도** \n",
    "\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759578-3e1a4a50-07fc-4a9d-9682-3f2a5a07ba7c.JPG\" alt=\"14\" width=\"700px\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxPFMMWQKOEY"
   },
   "source": [
    "## Implementation Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w2YtibfT8Zy"
   },
   "source": [
    "### [Implementation](https://github.com/rogertrullo/Gated-PixelCNN-Pytorch) (Unconditional Gated PixelCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference implementaion은 **padding/crop이 아니라 masking 방식으로 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1492,
     "status": "ok",
     "timestamp": 1648838691946,
     "user": {
      "displayName": "Sangkyu Lee",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "t1VU-bqwRVBd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "class MaskedConv(nn.Conv2d):\n",
    "    '''\n",
    "    Class that implements the masking for both streams vertical and horizontal.\n",
    "    It is different if it is the first layer (A) or subsequent layers (B)\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, mask_type='A', ver_or_hor='V', use_gpu=True):\n",
    "        assert mask_type in ['A', 'B'], 'only A or B are possible mask types'\n",
    "        assert ver_or_hor in ['V', 'H'], 'only H or V are possible ver_or_hor types'\n",
    "\n",
    "        if ver_or_hor == 'H':  # 1XN mask\n",
    "            pad = (0, (kernel_size - 1) // 2)\n",
    "            ksz = (1, kernel_size)\n",
    "\n",
    "        else:  # NxN mask vertical\n",
    "            ksz = kernel_size\n",
    "            pad = (kernel_size - 1) // 2\n",
    "\n",
    "        super().__init__(in_channels, out_channels, kernel_size=ksz, padding=pad)\n",
    "        self.mask = torch.zeros_like(self.weight)\n",
    "        if use_gpu:\n",
    "            self.mask = self.mask.cuda()#TODO make gpu optional\n",
    "\n",
    "        if mask_type == 'A':\n",
    "            if ver_or_hor == 'V':  # NXN mask\n",
    "                self.mask[:, :, 0:self.mask.shape[2] // 2, :] = 1\n",
    "\n",
    "            else:  # horizontal 1xN\n",
    "                self.mask[:, :, :, 0:self.mask.shape[3] // 2] = 1\n",
    "        else:  # B\n",
    "            if ver_or_hor == 'V':  # NXN mask\n",
    "                self.mask[:, :, 0:self.mask.shape[2] // 2, :] = 1\n",
    "                self.mask[:, :, self.mask.shape[2] // 2, :] = 1\n",
    "\n",
    "            else:  # horizontal 1xN\n",
    "                self.mask[:, :, :, 0:self.mask.shape[3] // 2 + 1] = 1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.weight.data *= self.mask  # mask weights\n",
    "        # print(self.weight)\n",
    "        return super().__call__(x)\n",
    "\n",
    "\n",
    "class GatedConvLayer(nn.Module):\n",
    "    '''\n",
    "    Main building block of the framework. It implements figure 2 of the paper.\n",
    "    '''\n",
    "    def __init__(self, in_channels, nfeats, kernel_size=3, mask_type='A'):\n",
    "        super(GatedConvLayer, self).__init__()\n",
    "        self.nfeats = nfeats\n",
    "        self.mask_type = mask_type\n",
    "        self.vconv = MaskedConv(in_channels=in_channels, out_channels=2 * nfeats, kernel_size=kernel_size,\n",
    "                                ver_or_hor='V', mask_type=mask_type)\n",
    "\n",
    "        self.hconv = MaskedConv(in_channels=in_channels, out_channels=2 * nfeats, kernel_size=kernel_size,\n",
    "                                ver_or_hor='H', mask_type=mask_type)\n",
    "\n",
    "        self.v_to_h_conv = nn.Conv2d(in_channels=2 * nfeats, out_channels=2 * nfeats, kernel_size=1)  # 1x1 conv\n",
    "\n",
    "        self.h_to_h_conv = nn.Conv2d(in_channels=nfeats, out_channels=nfeats, kernel_size=1)  # 1x1 conv\n",
    "\n",
    "    def GatedActivation(self, x):\n",
    "        return torch.tanh(x[:, :self.nfeats]) * torch.sigmoid(x[:, self.nfeats:])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be a list of two elements [v, h]\n",
    "        iv, ih = x\n",
    "        ov = self.vconv(iv)\n",
    "        oh_ = self.hconv(ih)\n",
    "        v2h = self.v_to_h_conv(ov)\n",
    "        oh = v2h + oh_\n",
    "\n",
    "        ov = self.GatedActivation(ov)\n",
    "\n",
    "        oh = self.GatedActivation(oh)\n",
    "        oh = self.h_to_h_conv(oh)\n",
    "\n",
    "        ##############################################################################\n",
    "        #Due to the residual connection, if we add it from the first layer, ##########\n",
    "        #the current pixel is included, in my implementation I removed the first #####\n",
    "        #residual connection to solve this issue #####################################\n",
    "        ##############################################################################\n",
    "        if self.mask_type == 'B':\n",
    "            oh = oh + ih\n",
    "\n",
    "        return [ov, oh]\n",
    "\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    '''\n",
    "    Class that stacks several GatedConvLayers, the output has Klevel maps.\n",
    "    Klevels indicates the number of possible values that a pixel can have e.g 2 for binary images or\n",
    "    256 for gray level imgs.\n",
    "    '''\n",
    "    def __init__(self, nlayers, in_channels, nfeats, Klevels=2, ksz_A=5, ksz_B=3):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GatedConvLayer(in_channels=in_channels, nfeats=nfeats, mask_type='A', kernel_size=ksz_A)])\n",
    "        for i in range(nlayers):\n",
    "            gatedconv = GatedConvLayer(in_channels=nfeats, nfeats=nfeats, mask_type='B', kernel_size=ksz_B)\n",
    "            self.layers.append(gatedconv)\n",
    "        #TODO make kernel sizes as params\n",
    "\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(nfeats, nfeats, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(nfeats, Klevels, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = [x, x]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        logits = self.out_conv(x[1])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g9CBFXgUuNR"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1648838691947,
     "user": {
      "displayName": "Sangkyu Lee",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "-h57mg9Abas9"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_workers=2\n",
    "\n",
    "show_every=100 # show info every this number of iterations\n",
    "nlayers=12 # number of layers for pixelcnn\n",
    "inchans=1 #number of input channels (currently only one is supported)\n",
    "nfeats=16 #number of feature maps across the network\n",
    "Klevels=4 #number of levels to use in discretization\n",
    "nepochs=5 #number of epochs to train\n",
    "lr=1e-3 #learning rate for optimizer\n",
    "generate_every=300\n",
    "nimgs_to_generate=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1648838691947,
     "user": {
      "displayName": "Sangkyu Lee",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "IM0E4TF7bnUz"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = MNIST('./data', download=True, train=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1648838691948,
     "user": {
      "displayName": "Sangkyu Lee",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "IwbMOcilasCC"
   },
   "outputs": [],
   "source": [
    "def generate_imgs(model, shape, nimgs):\n",
    "    x=torch.zeros((nimgs,1,shape[0],shape[1])).cuda()\n",
    "    logits=model(x)\n",
    "    model.eval()\n",
    "    for i in range(x.shape[2]):\n",
    "        for j in range(x.shape[3]):\n",
    "            logits=model(x)\n",
    "            probs=torch.softmax(logits[:,:,i,j],1)\n",
    "            sample=probs.multinomial(1)\n",
    "            x[:,:,i,j]=sample.float()/(Klevels-1)\n",
    "    model.train()\n",
    "    return x.cpu()\n",
    "\n",
    "def discretize_imgs(img_tensor, nlevels):\n",
    "    '''\n",
    "    discretize a floating tensor to a discrete version into nlevels (quantization).\n",
    "    The function assumes that the data is between [0,1]\n",
    "    it return the two outputs; the first is again between [0,1] but only nlevels.\n",
    "    The second is the equivalente but with integer indices between [0,nlevels-1]\n",
    "    '''\n",
    "    xnp=img_tensor.numpy()\n",
    "    xnp_dig=(np.digitize(xnp, np.arange(nlevels) / nlevels) - 1).astype(np.long)\n",
    "    xnp=xnp_dig/(nlevels -1)\n",
    "    return torch.from_numpy(xnp).float(), torch.from_numpy(xnp_dig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386235,
     "status": "ok",
     "timestamp": 1648839078179,
     "user": {
      "displayName": "Sangkyu Lee",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "_CUYQMY9aJIz",
    "outputId": "85da0771-181b-4020-d319-f9133c7bfd16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, it:0/938, loss:1.3460774421691895\n",
      "generating imgs...\n",
      "epoch: 0, it:100/938, loss:0.22751770913600922\n",
      "epoch: 0, it:200/938, loss:0.19475848972797394\n",
      "epoch: 0, it:300/938, loss:0.19172492623329163\n",
      "generating imgs...\n",
      "epoch: 0, it:400/938, loss:0.18021661043167114\n",
      "epoch: 0, it:500/938, loss:0.19130343198776245\n",
      "epoch: 0, it:600/938, loss:0.1697963923215866\n",
      "generating imgs...\n",
      "epoch: 0, it:700/938, loss:0.18709248304367065\n",
      "epoch: 0, it:800/938, loss:0.18620853126049042\n",
      "epoch: 0, it:900/938, loss:0.16931068897247314\n",
      "generating imgs...\n",
      "epoch: 1, it:0/938, loss:0.1788172423839569\n",
      "generating imgs...\n",
      "epoch: 1, it:100/938, loss:0.16182249784469604\n",
      "epoch: 1, it:200/938, loss:0.1640923172235489\n",
      "epoch: 1, it:300/938, loss:0.15908195078372955\n",
      "generating imgs...\n",
      "epoch: 1, it:400/938, loss:0.15759702026844025\n",
      "epoch: 1, it:500/938, loss:0.16653862595558167\n",
      "epoch: 1, it:600/938, loss:0.1695224642753601\n",
      "generating imgs...\n",
      "epoch: 1, it:700/938, loss:0.16781367361545563\n",
      "epoch: 1, it:800/938, loss:0.16412104666233063\n",
      "epoch: 1, it:900/938, loss:0.1615515947341919\n",
      "generating imgs...\n",
      "epoch: 2, it:0/938, loss:0.16497518122196198\n",
      "generating imgs...\n",
      "epoch: 2, it:100/938, loss:0.16784439980983734\n",
      "epoch: 2, it:200/938, loss:0.1586044728755951\n",
      "epoch: 2, it:300/938, loss:0.16359488666057587\n",
      "generating imgs...\n",
      "epoch: 2, it:400/938, loss:0.16425305604934692\n",
      "epoch: 2, it:500/938, loss:0.15541943907737732\n",
      "epoch: 2, it:600/938, loss:0.15232622623443604\n",
      "generating imgs...\n",
      "epoch: 2, it:700/938, loss:0.14880797266960144\n",
      "epoch: 2, it:800/938, loss:0.14279280602931976\n",
      "epoch: 2, it:900/938, loss:0.14372609555721283\n",
      "generating imgs...\n",
      "epoch: 3, it:0/938, loss:0.15871374309062958\n",
      "generating imgs...\n",
      "epoch: 3, it:100/938, loss:0.1514120250940323\n",
      "epoch: 3, it:200/938, loss:0.15867477655410767\n",
      "epoch: 3, it:300/938, loss:0.14681291580200195\n",
      "generating imgs...\n",
      "epoch: 3, it:400/938, loss:0.15638674795627594\n",
      "epoch: 3, it:500/938, loss:0.16007192432880402\n",
      "epoch: 3, it:600/938, loss:0.15759843587875366\n",
      "generating imgs...\n",
      "epoch: 3, it:700/938, loss:0.15716534852981567\n",
      "epoch: 3, it:800/938, loss:0.14957356452941895\n",
      "epoch: 3, it:900/938, loss:0.1504368633031845\n",
      "generating imgs...\n",
      "epoch: 4, it:0/938, loss:0.14374838769435883\n",
      "generating imgs...\n",
      "epoch: 4, it:100/938, loss:0.14708365499973297\n",
      "epoch: 4, it:200/938, loss:0.1495913416147232\n",
      "epoch: 4, it:300/938, loss:0.15203726291656494\n",
      "generating imgs...\n",
      "epoch: 4, it:400/938, loss:0.15513299405574799\n",
      "epoch: 4, it:500/938, loss:0.14649386703968048\n",
      "epoch: 4, it:600/938, loss:0.1534404158592224\n",
      "generating imgs...\n",
      "epoch: 4, it:700/938, loss:0.15314821898937225\n",
      "epoch: 4, it:800/938, loss:0.15234427154064178\n",
      "epoch: 4, it:900/938, loss:0.13963636755943298\n",
      "generating imgs...\n"
     ]
    }
   ],
   "source": [
    "model = PixelCNN(nlayers=nlayers, in_channels=inchans, nfeats=nfeats, Klevels=Klevels).cuda()\n",
    "optimizer = Adam(model.parameters(), lr=lr, betas=(0, 0.99))\n",
    "criteria=nn.CrossEntropyLoss()\n",
    "\n",
    "list_imgs=[]\n",
    "for epoch in range(nepochs):\n",
    "    for it,(images, labels) in enumerate(train_loader):\n",
    "\n",
    "        imgs,imgs_quant= discretize_imgs(images, Klevels)\n",
    "        imgs=imgs.cuda()\n",
    "        imgs_quant=imgs_quant.cuda()\n",
    "        logits=model(imgs)\n",
    "        loss=criteria(logits,imgs_quant.squeeze())\n",
    "        optimizer.zero_grad() # Backward & update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if it%show_every==0:\n",
    "            print(f'epoch: {epoch}, it:{it}/{len(train_loader)}, loss:{loss.item()}')\n",
    "        if it%generate_every==0:\n",
    "            print('generating imgs...')\n",
    "            samples=generate_imgs(model, (imgs.shape[2],imgs.shape[3]), nimgs_to_generate)\n",
    "            list_imgs.append(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hkeM1CRU3W0"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15816,
     "status": "ok",
     "timestamp": 1648839327593,
     "user": {
      "displayName": "Sangkyu Lee",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "HVW5hwQwepep",
    "outputId": "15590e15-5180-44d9-e87c-afe3738e98fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  fonts-droid-fallback fonts-noto-mono ghostscript gsfonts\n",
      "  imagemagick-6-common imagemagick-6.q16 libcupsfilters1 libcupsimage2\n",
      "  libdjvulibre-text libdjvulibre21 libgs9 libgs9-common libijs-0.35\n",
      "  libjbig2dec0 liblqr-1-0 libmagickcore-6.q16-3 libmagickcore-6.q16-3-extra\n",
      "  libmagickwand-6.q16-3 libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
      "Suggested packages:\n",
      "  fonts-noto ghostscript-x imagemagick-doc autotrace cups-bsd | lpr | lprng\n",
      "  enscript gimp gnuplot grads hp2xx html2ps libwmf-bin mplayer povray radiance\n",
      "  sane-utils texlive-base-bin transfig ufraw-batch inkscape libjxr-tools\n",
      "  libwmf0.2-7-gtk poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
      "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
      "  fonts-arphic-uming fonts-nanum\n",
      "The following NEW packages will be installed:\n",
      "  fonts-droid-fallback fonts-noto-mono ghostscript gsfonts imagemagick\n",
      "  imagemagick-6-common imagemagick-6.q16 libcupsfilters1 libcupsimage2\n",
      "  libdjvulibre-text libdjvulibre21 libgs9 libgs9-common libijs-0.35\n",
      "  libjbig2dec0 liblqr-1-0 libmagickcore-6.q16-3 libmagickcore-6.q16-3-extra\n",
      "  libmagickwand-6.q16-3 libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
      "0 upgraded, 23 newly installed, 0 to remove and 39 not upgraded.\n",
      "Need to get 18.4 MB of archives.\n",
      "After this operation, 66.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblqr-1-0 amd64 0.4.2-2.1 [27.7 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick-6-common all 8:6.9.7.4+dfsg-16ubuntu6.12 [60.3 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickcore-6.q16-3 amd64 8:6.9.7.4+dfsg-16ubuntu6.12 [1,621 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickwand-6.q16-3 amd64 8:6.9.7.4+dfsg-16ubuntu6.12 [292 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.15 [5,092 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [2,265 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.15 [51.4 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick-6.q16 amd64 8:6.9.7.4+dfsg-16ubuntu6.12 [423 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 imagemagick amd64 8:6.9.7.4+dfsg-16ubuntu6.12 [14.2 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdjvulibre-text all 3.5.27.1-8ubuntu0.4 [49.4 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdjvulibre21 amd64 3.5.27.1-8ubuntu0.4 [561 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwmf0.2-7 amd64 0.2.8.4-12 [150 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagickcore-6.q16-3-extra amd64 8:6.9.7.4+dfsg-16ubuntu6.12 [62.4 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnetpbm10 amd64 2:10.0-15.3build1 [58.0 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 netpbm amd64 2:10.0-15.3build1 [1,017 kB]\n",
      "Fetched 18.4 MB in 1s (12.3 MB/s)\n",
      "Selecting previously unselected package fonts-droid-fallback.\n",
      "(Reading database ... 156210 files and directories currently installed.)\n",
      "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
      "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
      "Selecting previously unselected package liblqr-1-0:amd64.\n",
      "Preparing to unpack .../01-liblqr-1-0_0.4.2-2.1_amd64.deb ...\n",
      "Unpacking liblqr-1-0:amd64 (0.4.2-2.1) ...\n",
      "Selecting previously unselected package imagemagick-6-common.\n",
      "Preparing to unpack .../02-imagemagick-6-common_8%3a6.9.7.4+dfsg-16ubuntu6.12_all.deb ...\n",
      "Unpacking imagemagick-6-common (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Selecting previously unselected package libmagickcore-6.q16-3:amd64.\n",
      "Preparing to unpack .../03-libmagickcore-6.q16-3_8%3a6.9.7.4+dfsg-16ubuntu6.12_amd64.deb ...\n",
      "Unpacking libmagickcore-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Selecting previously unselected package libmagickwand-6.q16-3:amd64.\n",
      "Preparing to unpack .../04-libmagickwand-6.q16-3_8%3a6.9.7.4+dfsg-16ubuntu6.12_amd64.deb ...\n",
      "Unpacking libmagickwand-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Selecting previously unselected package poppler-data.\n",
      "Preparing to unpack .../05-poppler-data_0.4.8-2_all.deb ...\n",
      "Unpacking poppler-data (0.4.8-2) ...\n",
      "Selecting previously unselected package fonts-noto-mono.\n",
      "Preparing to unpack .../06-fonts-noto-mono_20171026-2_all.deb ...\n",
      "Unpacking fonts-noto-mono (20171026-2) ...\n",
      "Selecting previously unselected package libcupsimage2:amd64.\n",
      "Preparing to unpack .../07-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
      "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
      "Selecting previously unselected package libijs-0.35:amd64.\n",
      "Preparing to unpack .../08-libijs-0.35_0.35-13_amd64.deb ...\n",
      "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
      "Selecting previously unselected package libjbig2dec0:amd64.\n",
      "Preparing to unpack .../09-libjbig2dec0_0.13-6_amd64.deb ...\n",
      "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
      "Selecting previously unselected package libgs9-common.\n",
      "Preparing to unpack .../10-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.15_all.deb ...\n",
      "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
      "Selecting previously unselected package libgs9:amd64.\n",
      "Preparing to unpack .../11-libgs9_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
      "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
      "Selecting previously unselected package ghostscript.\n",
      "Preparing to unpack .../12-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.15_amd64.deb ...\n",
      "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
      "Selecting previously unselected package gsfonts.\n",
      "Preparing to unpack .../13-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
      "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
      "Selecting previously unselected package imagemagick-6.q16.\n",
      "Preparing to unpack .../14-imagemagick-6.q16_8%3a6.9.7.4+dfsg-16ubuntu6.12_amd64.deb ...\n",
      "Unpacking imagemagick-6.q16 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Selecting previously unselected package imagemagick.\n",
      "Preparing to unpack .../15-imagemagick_8%3a6.9.7.4+dfsg-16ubuntu6.12_amd64.deb ...\n",
      "Unpacking imagemagick (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Selecting previously unselected package libcupsfilters1:amd64.\n",
      "Preparing to unpack .../16-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
      "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
      "Selecting previously unselected package libdjvulibre-text.\n",
      "Preparing to unpack .../17-libdjvulibre-text_3.5.27.1-8ubuntu0.4_all.deb ...\n",
      "Unpacking libdjvulibre-text (3.5.27.1-8ubuntu0.4) ...\n",
      "Selecting previously unselected package libdjvulibre21:amd64.\n",
      "Preparing to unpack .../18-libdjvulibre21_3.5.27.1-8ubuntu0.4_amd64.deb ...\n",
      "Unpacking libdjvulibre21:amd64 (3.5.27.1-8ubuntu0.4) ...\n",
      "Selecting previously unselected package libwmf0.2-7:amd64.\n",
      "Preparing to unpack .../19-libwmf0.2-7_0.2.8.4-12_amd64.deb ...\n",
      "Unpacking libwmf0.2-7:amd64 (0.2.8.4-12) ...\n",
      "Selecting previously unselected package libmagickcore-6.q16-3-extra:amd64.\n",
      "Preparing to unpack .../20-libmagickcore-6.q16-3-extra_8%3a6.9.7.4+dfsg-16ubuntu6.12_amd64.deb ...\n",
      "Unpacking libmagickcore-6.q16-3-extra:amd64 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Selecting previously unselected package libnetpbm10.\n",
      "Preparing to unpack .../21-libnetpbm10_2%3a10.0-15.3build1_amd64.deb ...\n",
      "Unpacking libnetpbm10 (2:10.0-15.3build1) ...\n",
      "Selecting previously unselected package netpbm.\n",
      "Preparing to unpack .../22-netpbm_2%3a10.0-15.3build1_amd64.deb ...\n",
      "Unpacking netpbm (2:10.0-15.3build1) ...\n",
      "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
      "Setting up imagemagick-6-common (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
      "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
      "Setting up poppler-data (0.4.8-2) ...\n",
      "Setting up libdjvulibre-text (3.5.27.1-8ubuntu0.4) ...\n",
      "Setting up libnetpbm10 (2:10.0-15.3build1) ...\n",
      "Setting up fonts-noto-mono (20171026-2) ...\n",
      "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
      "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
      "Setting up liblqr-1-0:amd64 (0.4.2-2.1) ...\n",
      "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
      "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
      "Setting up netpbm (2:10.0-15.3build1) ...\n",
      "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
      "Setting up libwmf0.2-7:amd64 (0.2.8.4-12) ...\n",
      "Setting up libmagickcore-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Setting up libdjvulibre21:amd64 (3.5.27.1-8ubuntu0.4) ...\n",
      "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.15) ...\n",
      "Setting up libmagickwand-6.q16-3:amd64 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Setting up imagemagick-6.q16 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare (compare) in auto mode\n",
      "update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare-im6 (compare-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate (animate) in auto mode\n",
      "update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate-im6 (animate-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert (convert) in auto mode\n",
      "update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert-im6 (convert-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite (composite) in auto mode\n",
      "update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite-im6 (composite-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure (conjure) in auto mode\n",
      "update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure-im6 (conjure-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import (import) in auto mode\n",
      "update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import-im6 (import-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify (identify) in auto mode\n",
      "update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify-im6 (identify-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream (stream) in auto mode\n",
      "update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream-im6 (stream-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display (display) in auto mode\n",
      "update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display-im6 (display-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage (montage) in auto mode\n",
      "update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage-im6 (montage-im6) in auto mode\n",
      "update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify (mogrify) in auto mode\n",
      "update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify-im6 (mogrify-im6) in auto mode\n",
      "Setting up libmagickcore-6.q16-3-extra:amd64 (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Setting up imagemagick (8:6.9.7.4+dfsg-16ubuntu6.12) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt install imagemagick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25845,
     "status": "ok",
     "timestamp": 1648839374171,
     "user": {
      "displayName": "Sangkyu Lee",
      "userId": "09682951002772598371"
     },
     "user_tz": -540
    },
    "id": "ZXf6DAdsdP6o",
    "outputId": "93dc88f7-f066-4541-a316-f4dd6d3dcf9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "frame 0\n",
      "frame 0\n",
      "frame 1\n",
      "frame 2\n",
      "frame 3\n",
      "frame 4\n",
      "frame 5\n",
      "frame 6\n",
      "frame 7\n",
      "frame 8\n",
      "frame 9\n",
      "frame 10\n",
      "frame 11\n",
      "frame 12\n",
      "frame 13\n",
      "frame 14\n",
      "frame 15\n",
      "frame 16\n",
      "frame 17\n",
      "frame 18\n",
      "frame 19\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "print(len(list_imgs))\n",
    "fig, ax=plt.subplots(figsize=(20,20))\n",
    "\n",
    "def gen_grid(i):\n",
    "    print(f'frame {i}')\n",
    "    img=make_grid(list_imgs[i], nrow=4)\n",
    "    npimg = img.numpy()\n",
    "    ax.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    ax.set_title(f'iteration {i*generate_every}', fontsize=50)\n",
    "    \n",
    "  \n",
    "anim = FuncAnimation(fig, gen_grid, frames=np.arange(len(list_imgs)), interval=500)\n",
    "anim.save('digits.gif', dpi=80, writer='imagemagick')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8qphiLFfSpA"
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/86907286/161759581-0d88aff6-aa97-407e-af78-c365057a8782.gif\" alt=\"digits\" width=\"500px\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ak0Dn-LURm2"
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahcpUlB9UUpB"
   },
   "source": [
    "https://arxiv.org/pdf/1601.06759.pdf  \n",
    "https://arxiv.org/pdf/1606.05328v2.pdf  \n",
    "https://sergeiturukin.com/2017/02/24/gated-pixelcnn.html  \n",
    "https://youtu.be/1BURwCCYNEI  \n",
    "https://github.com/rogertrullo/Gated-PixelCNN-Pytorch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOy6sMktL3Wiz8Wfsn1WpYv",
   "collapsed_sections": [
    "uxPFMMWQKOEY",
    "-w2YtibfT8Zy",
    "5g9CBFXgUuNR",
    "7hkeM1CRU3W0"
   ],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
